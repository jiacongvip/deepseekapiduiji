from fastapi import APIRouter, Body, Query, HTTPException, Header
from fastapi.responses import StreamingResponse
from src.service import chat_completion, delete_conversation
from src.model.response import CompletionResponse, DeleteResponse
from src.model.request import CompletionRequest
from src.pool.session_pool import DoubaoSession
from typing import Optional
from loguru import logger
import json
import uuid
import time


router = APIRouter()


def format_references_as_markdown(references: list) -> str:
    """å°†å¼•ç”¨åˆ—è¡¨æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ï¼Œé™„åŠ åˆ°å†…å®¹æœ«å°¾"""
    if not references:
        return ""
    
    lines = ["\n\n---\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n"]
    for i, ref in enumerate(references, 1):
        title = ref.get('title', 'æœªçŸ¥æ ‡é¢˜')
        url = ref.get('url', '')
        sitename = ref.get('sitename', '')
        
        if url:
            # Markdown é“¾æ¥æ ¼å¼
            site_info = f" ({sitename})" if sitename else ""
            lines.append(f"{i}. [{title}]({url}){site_info}")
        else:
            lines.append(f"{i}. {title}")
    
    return "\n".join(lines)


def create_openai_response(text: str, conv_id: str = "", model: str = "doubao-pro-4k", references: list = None):
    """åˆ›å»º OpenAI å…¼å®¹çš„éæµå¼å“åº”"""
    message = {
        "role": "assistant",
        "content": text
    }
    
    # å¦‚æœæœ‰å¼•ç”¨ï¼Œæ·»åŠ åˆ° message ä¸­
    if references:
        message["references"] = references
    
    return {
        "id": f"chatcmpl-{conv_id or uuid.uuid4()}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": message,
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": len(text),
            "total_tokens": len(text)
        }
    }


def create_openai_stream_chunk(content: str, conv_id: str = "", model: str = "doubao-pro-4k", is_first: bool = False, is_done: bool = False, references: list = None):
    """åˆ›å»º OpenAI å…¼å®¹çš„æµå¼å“åº” chunk"""
    if is_done:
        delta = {}
        # å¦‚æœæœ‰å¼•ç”¨ï¼Œåœ¨ç»“æŸæ—¶å‘é€
        if references:
            delta["references"] = references
        return {
            "id": f"chatcmpl-{conv_id or uuid.uuid4()}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {
                    "index": 0,
                    "delta": delta,
                    "finish_reason": "stop"
                }
            ]
        }
    
    delta = {"content": content}
    if is_first:
        delta["role"] = "assistant"
    
    return {
        "id": f"chatcmpl-{conv_id or uuid.uuid4()}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": delta,
                "finish_reason": None
            }
        ]
    }


@router.post("/completions")
async def api_completions(
    completion: CompletionRequest = Body(),
    authorization: Optional[str] = Header(None)
):
    """
    è±†åŒ…èŠå¤©è¡¥å…¨æ¥å£ - OpenAI å…¼å®¹æ ¼å¼
    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    """
    session_override = None
    if authorization:
        try:
            token_str = authorization.replace("Bearer ", "").strip()
            if token_str.startswith("{") and token_str.endswith("}"):
                session_data = json.loads(token_str)
                if "cookie" in session_data:
                    session_override = DoubaoSession(
                        cookie=session_data.get("cookie", ""),
                        device_id=session_data.get("device_id", "0"),
                        tea_uuid=session_data.get("tea_uuid", "0"),
                        web_id=session_data.get("web_id", "0"),
                        room_id=session_data.get("room_id", "0"),
                        x_flow_trace=session_data.get("x_flow_trace", "")
                    )
        except Exception as e:
            logger.warning(f"Failed to parse Authorization header as session config: {e}")

    # Convert OpenAI messages to prompt
    prompt = completion.prompt
    if not prompt and completion.messages:
        for msg in reversed(completion.messages):
            if msg.get("role") == "user":
                content = msg.get("content")
                # Handle multimodal content (list format)
                if isinstance(content, list):
                    text_parts = []
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_parts.append(item.get("text", ""))
                        elif isinstance(item, str):
                            text_parts.append(item)
                    prompt = "\n".join(text_parts)
                else:
                    prompt = content
                break
    
    if not prompt:
        raise HTTPException(status_code=400, detail="Field 'prompt' or 'messages' (with user content) is required")

    model = completion.model or "doubao-pro-4k"
    stream = completion.stream or False
    
    # æ£€æŸ¥æ¨¡å‹åæ˜¯å¦åŒ…å« deepï¼Œå¯ç”¨æ·±åº¦æ€è€ƒ
    use_deep_think = completion.use_deep_think or ("deep" in model.lower())
    
    logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: prompté•¿åº¦={len(prompt)}, stream={stream}, model={model}, use_deep_think={use_deep_think}")

    if stream:
        # æµå¼å“åº”
        async def generate_stream():
            try:
                text, imgs, refs, conv_id, msg_id, sec_id = await chat_completion(
            prompt=prompt,
            guest=completion.guest,
            conversation_id=completion.conversation_id,
            section_id=completion.section_id,
            attachments=completion.attachments,
            use_auto_cot=completion.use_auto_cot,
                    use_deep_think=use_deep_think,
            session_override=session_override
                )
                
                logger.info(f"èŠå¤©å®Œæˆ: texté•¿åº¦={len(text) if text else 0}, å¼•ç”¨æ•°={len(refs) if refs else 0}")
                
                if text:
                    # å‘é€ç¬¬ä¸€ä¸ª chunkï¼ˆåŒ…å« roleï¼‰
                    first_chunk = create_openai_stream_chunk("", conv_id, model, is_first=True)
                    yield f"data: {json.dumps(first_chunk)}\n\n"
                    
                    # å°†å¼•ç”¨æ ¼å¼åŒ–ä¸º Markdown å¹¶é™„åŠ åˆ°å†…å®¹æœ«å°¾
                    full_text = text
                    if refs:
                        full_text += format_references_as_markdown(refs)
                    
                    # åˆ†å—å‘é€å†…å®¹ï¼ˆæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼‰
                    chunk_size = 10  # æ¯æ¬¡å‘é€çš„å­—ç¬¦æ•°
                    for i in range(0, len(full_text), chunk_size):
                        chunk_text = full_text[i:i+chunk_size]
                        chunk = create_openai_stream_chunk(chunk_text, conv_id, model)
                        yield f"data: {json.dumps(chunk)}\n\n"
                    
                    # å‘é€ç»“æŸ chunkï¼ˆä¹Ÿé™„å¸¦å¼•ç”¨å…ƒæ•°æ®ï¼Œä¾›æ”¯æŒçš„å®¢æˆ·ç«¯ä½¿ç”¨ï¼‰
                    done_chunk = create_openai_stream_chunk("", conv_id, model, is_done=True, references=refs)
                    yield f"data: {json.dumps(done_chunk)}\n\n"
                else:
                    # ç©ºå“åº”
                    error_chunk = create_openai_stream_chunk("æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°å›å¤å†…å®¹ã€‚", conv_id, model, is_first=True)
                    yield f"data: {json.dumps(error_chunk)}\n\n"
                    done_chunk = create_openai_stream_chunk("", conv_id, model, is_done=True)
                    yield f"data: {json.dumps(done_chunk)}\n\n"
                
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                logger.error(f"æµå¼å“åº”é”™è¯¯: {e}", exc_info=True)
                error_response = {
                    "error": {
                        "message": str(e),
                        "type": "server_error",
                        "code": 500
                    }
                }
                yield f"data: {json.dumps(error_response)}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "Access-Control-Allow-Origin": "*",
            }
        )
    else:
        # éæµå¼å“åº”
        try:
            text, imgs, refs, conv_id, msg_id, sec_id = await chat_completion(
                prompt=prompt,
                guest=completion.guest,
                conversation_id=completion.conversation_id,
                section_id=completion.section_id,
                attachments=completion.attachments,
                use_auto_cot=completion.use_auto_cot,
                use_deep_think=use_deep_think,
                session_override=session_override
            )
            
            logger.info(f"èŠå¤©å®Œæˆ: texté•¿åº¦={len(text) if text else 0}, å¼•ç”¨æ•°={len(refs) if refs else 0}, conversation_id={conv_id}")
            
            if not text:
                text = "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°å›å¤å†…å®¹ã€‚"
            
            # å°†å¼•ç”¨æ ¼å¼åŒ–ä¸º Markdown å¹¶é™„åŠ åˆ°å†…å®¹æœ«å°¾
            full_text = text
            if refs:
                full_text += format_references_as_markdown(refs)
            
            return create_openai_response(full_text, conv_id, model, references=refs)
            
        except Exception as e:
            logger.error(f"èŠå¤©è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=str(e))



@router.post("/delete", response_model=DeleteResponse)
async def api_delete(conversation_id: str = Query()):
    """
    åˆ é™¤èŠå¤©
    1. conversation_id ä¸å­˜åœ¨ä¹Ÿä¼šæç¤ºæˆåŠŸ
    2. å»ºè®®åœ¨èŠå¤©ç»“æŸæ—¶éƒ½è°ƒç”¨å‡½æ•°ï¼Œé¿å…åˆ›å»ºè¿‡å¤šå¯¹è¯
    """
    try:
        ok, msg = await delete_conversation(conversation_id)
        return DeleteResponse(
            ok=ok,
            msg=msg
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))